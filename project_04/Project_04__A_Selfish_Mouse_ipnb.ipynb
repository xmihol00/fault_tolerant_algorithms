{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIG3sDBFyebw"
      },
      "source": [
        "# Project 4 -- A Selfish Mouse\n",
        "\n",
        "## Instructions\n",
        "\n",
        "Please read carefully:\n",
        "\n",
        "* Solve the project yourself. No teamwork.\n",
        "* If you have questions, please post these in the public channel on Slack. The answers may be relevant to others as well. \n",
        "* Feel free to import and use any additional Python package you need.\n",
        "* You are allowed to solve the project using a different programming language (although this is not adviced, since the provided basis implementation is in Python).\n",
        "* Your code may be tested on other world layouts (you are provided with two: `world_empty.txt` and `world_walls.txt` for experimentation).\n",
        "* The refresh of the game grid is slow in Colab (and results in a noticible fluttering). To fix the performance issue, run the GitHub (not the Colab) implementation locally (provided here: [Basic Reinforcement Learning](https://github.com/vmayoral/basic_reinforcement_learning/tree/master/tutorial1)). This resolves the problem. There is no difference if you solve this project locally or in Colab. The GitHub code was adjusted to run in Colab to simplify things (and with a hope that Colab performance issues will be fixed at some point in the future, since this is a known issue and there are folks working on this right now).\n",
        "* Make sure to fill in your `student_name` in the following block below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NRQm4C5L0AsG"
      },
      "outputs": [],
      "source": [
        "student_name = 'David Mihola' # fill with your student name\n",
        "assert student_name != 'your_student_name', 'Please fill in your student_name before you start.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmM9HxEAJCdX"
      },
      "source": [
        "## Setup\n",
        "\n",
        "In this project you will gain practical experience with a Q-learning algorithm in a multi-agent environment. You are provided a grid world game where a Q-learning algorithm teaches a mouse to find its way to a piece of cheese while avoiding a cat on its way.\n",
        "\n",
        "<img src=\"https://st3.depositphotos.com/1000152/12958/i/450/depositphotos_129589806-stock-photo-little-rat-eating-cheese.jpg\" width='300' align='center'>\n",
        "\n",
        "Read the code below and understand it. The code is based on the [Basic Reinforcement Learning](https://github.com/vmayoral/basic_reinforcement_learning/tree/master/tutorial1) tutorial. The code below is an adaptation of the GibHub project to run in Google Colab. You can run the GitHub code locally on your machine (and it will be 10x faster!) The project is solvable in Colab though.\n",
        "\n",
        "Observe that a mouse is eaten by a cat over 10 times more often than it gets the cheese. Your task is to modify **only the Mouse class** implementation to help the mouse get the cheese. You are encouraged to try different ideas and see what works and what doesn’t (=design a selfish mouse!). Don’t expect to ”win” in this game: Feeding the mouse 100% of the times will not be possible, but you can improve mouse performance.\n",
        "\n",
        "Necessary installs and imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HIkEiE-f9H1S"
      },
      "outputs": [],
      "source": [
        "#!pip install pygame\n",
        "#!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ypb5Kp5vqiLG"
      },
      "outputs": [],
      "source": [
        "#!gdown https://drive.google.com/file/d/1osZDerlQk98wp-OVKsT8tkFfce310fMf/view?usp=sharing --fuzzy\n",
        "#!unzip world_setup.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moW7HmnT2Pfs"
      },
      "source": [
        "Q-learning implementation (taken from [Basic Reinforcement Learning](https://github.com/vmayoral/basic_reinforcement_learning/tree/master/tutorial1))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RqcwVk_uu1ac"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class QLearn:\n",
        "    def __init__(self, actions, epsilon=0.1, alpha=0.2, gamma=0.9):\n",
        "        self.q = {}\n",
        "\n",
        "        self.epsilon = epsilon  # exploration constant\n",
        "        self.alpha = alpha      # discount constant\n",
        "        self.gamma = gamma\n",
        "        self.actions = actions\n",
        "\n",
        "    def getQ(self, state, action):\n",
        "        return self.q.get((state, action), 0.0)\n",
        "        # return self.q.get((state, action), 1.0)\n",
        "\n",
        "    def learnQ(self, state, action, reward, value):\n",
        "        '''\n",
        "        Q-learning: Q(s, a) += alpha * (reward(s,a) + max(Q(s') - Q(s,a))\n",
        "        '''\n",
        "        oldv = self.q.get((state, action), None)\n",
        "        if oldv is None:\n",
        "            self.q[(state, action)] = reward\n",
        "        else:\n",
        "            self.q[(state, action)] = oldv + self.alpha * (value - oldv)\n",
        "\n",
        "    def chooseAction(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            action = random.choice(self.actions)\n",
        "        else:\n",
        "            q = [self.getQ(state, a) for a in self.actions]\n",
        "            maxQ = max(q)\n",
        "            count = q.count(maxQ)\n",
        "            # In case there're several state-action max values \n",
        "            # we select a random one among them\n",
        "            if count > 1:\n",
        "                best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
        "                i = random.choice(best)\n",
        "            else:\n",
        "                i = q.index(maxQ)\n",
        "\n",
        "            action = self.actions[i]\n",
        "        return action\n",
        "\n",
        "    def learn(self, state1, action1, reward, state2):\n",
        "        maxqnew = max([self.getQ(state2, a) for a in self.actions])\n",
        "        self.learnQ(state1, action1, reward, reward + self.gamma*maxqnew)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3xrilDm2h5h"
      },
      "source": [
        "Implementation of the Cheese, Cat and Mouse classes. **You are only allowed to modify the Mouse class and the related parameters**. You may want to copy the abive code when solving individual tasks or modify the code above. Please comment on how you achieved an improved performance of the mouse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "v5Vg50zTu9q_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000, e: 0.10, W: 48, L: 0\n",
            "20000, e: 0.10, W: 38, L: 0\n",
            "30000, e: 0.10, W: 40, L: 0\n",
            "40000, e: 0.10, W: 35, L: 0\n",
            "50000, e: 0.10, W: 47, L: 0\n",
            "60000, e: 0.10, W: 41, L: 0\n",
            "70000, e: 0.10, W: 32, L: 0\n",
            "80000, e: 0.10, W: 38, L: 0\n",
            "90000, e: 0.10, W: 27, L: 0\n",
            "100000, e: 0.10, W: 33, L: 0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/david/projs/fault_tolerant_algorithms/project_04/Project_04__A_Selfish_Mouse_ipnb.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 144>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/david/projs/fault_tolerant_algorithms/project_04/Project_04__A_Selfish_Mouse_ipnb.ipynb#X11sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m world\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mdelay \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/david/projs/fault_tolerant_algorithms/project_04/Project_04__A_Selfish_Mouse_ipnb.ipynb#X11sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/david/projs/fault_tolerant_algorithms/project_04/Project_04__A_Selfish_Mouse_ipnb.ipynb#X11sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m     world\u001b[39m.\u001b[39;49mupdate(mouse\u001b[39m.\u001b[39;49mfed, mouse\u001b[39m.\u001b[39;49meaten)\n",
            "File \u001b[0;32m~/projs/fault_tolerant_algorithms/project_04/cellular.py:219\u001b[0m, in \u001b[0;36mWorld.update\u001b[0;34m(self, fed, eaten)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m (eaten):\n\u001b[1;32m    218\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meaten \u001b[39m=\u001b[39m eaten\n\u001b[0;32m--> 219\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisplay\u001b[39m.\u001b[39;49mupdate()\n\u001b[1;32m    220\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
            "File \u001b[0;32m~/projs/fault_tolerant_algorithms/project_04/cellular.py:572\u001b[0m, in \u001b[0;36mPygameDisplay.update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    570\u001b[0m pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mflip()\n\u001b[1;32m    571\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelay \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 572\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdelay \u001b[39m*\u001b[39;49m \u001b[39m0.1\u001b[39;49m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import time\n",
        "import random\n",
        "import shelve\n",
        "import pdb\n",
        "\n",
        "import cellular\n",
        "\n",
        "directions = 8\n",
        "\n",
        "lookdist = 2\n",
        "lookcells = []\n",
        "for i in range(-lookdist,lookdist+1):\n",
        "    for j in range(-lookdist,lookdist+1):\n",
        "        if (abs(i) + abs(j) <= lookdist) and (i != 0 or j != 0):\n",
        "            lookcells.append((i,j))\n",
        "\n",
        "def pickRandomLocation():\n",
        "    while 1:\n",
        "        x = random.randrange(world.width)\n",
        "        y = random.randrange(world.height)\n",
        "        cell = world.getCell(x, y)\n",
        "        if not (cell.wall or len(cell.agents) > 0):\n",
        "            return cell\n",
        "\n",
        "\n",
        "class Cell(cellular.Cell):\n",
        "    wall = False\n",
        "\n",
        "    def colour(self):\n",
        "        if self.wall:\n",
        "            return 'black'\n",
        "        else:\n",
        "            return 'white'\n",
        "\n",
        "    def load(self, data):\n",
        "        if data == 'X':\n",
        "            self.wall = True\n",
        "        else:\n",
        "            self.wall = False\n",
        "\n",
        "\n",
        "class Cat(cellular.Agent):\n",
        "    cell = None\n",
        "    score = 0\n",
        "    colour = 'red'\n",
        "\n",
        "    def update(self):\n",
        "        cell = self.cell\n",
        "        if cell != mouse.cell:\n",
        "            self.goTowards(mouse.cell)\n",
        "            while cell == self.cell:\n",
        "                self.goInDirection(random.randrange(directions))\n",
        "\n",
        "\n",
        "class Cheese(cellular.Agent):\n",
        "    colour = 'green'\n",
        "\n",
        "    def update(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class Mouse(cellular.Agent):\n",
        "    colour = 'gray'\n",
        "\n",
        "    def __init__(self):\n",
        "        self.ai = None\n",
        "        self.ai = QLearn(actions=range(directions),\n",
        "                                alpha=0.1, gamma=0.9, epsilon=0.1)\n",
        "        self.eaten = 0\n",
        "        self.fed = 0\n",
        "        self.lastState = None\n",
        "        self.lastAction = None\n",
        "\n",
        "    def update(self):\n",
        "        # calculate the state of the surrounding cells\n",
        "        state = self.calcState()\n",
        "        # asign a reward of -1 by default\n",
        "        reward = -1\n",
        "\n",
        "        # observe the reward and update the Q-value\n",
        "        if self.cell == cat.cell:\n",
        "            self.eaten += 1\n",
        "            reward = -100\n",
        "            if self.lastState is not None:\n",
        "                self.ai.learn(self.lastState, self.lastAction, reward, state)\n",
        "            self.lastState = None\n",
        "\n",
        "            self.cell = pickRandomLocation()\n",
        "            return\n",
        "\n",
        "        if self.cell == cheese.cell:\n",
        "            self.fed += 1\n",
        "            reward = 50\n",
        "            cheese.cell = pickRandomLocation()\n",
        "\n",
        "        if self.lastState is not None:\n",
        "            self.ai.learn(self.lastState, self.lastAction, reward, state)\n",
        "\n",
        "        # Choose a new action and execute it\n",
        "        state = self.calcState()\n",
        "        # print(state)\n",
        "        action = self.ai.chooseAction(state)\n",
        "        self.lastState = state\n",
        "        self.lastAction = action\n",
        "\n",
        "        self.goInDirection(action)\n",
        "\n",
        "    def calcState(self):\n",
        "        def cellvalue(cell):\n",
        "            if cat.cell is not None and (cell.x == cat.cell.x and\n",
        "                                         cell.y == cat.cell.y):\n",
        "                return 3\n",
        "            elif cheese.cell is not None and (cell.x == cheese.cell.x and\n",
        "                                              cell.y == cheese.cell.y):\n",
        "                return 2\n",
        "            else:\n",
        "                return 1 if cell.wall else 0\n",
        "\n",
        "        return tuple([cellvalue(self.world.getWrappedCell(self.cell.x + j, self.cell.y + i))\n",
        "                      for i,j in lookcells])\n",
        "\n",
        "cheese = Cheese()\n",
        "mouse = Mouse()\n",
        "cat = Cat()\n",
        "\n",
        "world = cellular.World(Cell, directions=directions, filename='world_walls.txt')\n",
        "world.age = 0\n",
        "\n",
        "world.addAgent(cheese, cell=pickRandomLocation())\n",
        "#world.addAgent(cat)\n",
        "world.addAgent(mouse)\n",
        "\n",
        "endAge = world.age + 100000\n",
        "while world.age < endAge:\n",
        "    world.update()\n",
        "\n",
        "    if world.age % 10000 == 0:\n",
        "        print (\"{:d}, e: {:0.2f}, W: {:d}, L: {:d}\".format(world.age, mouse.ai.epsilon, mouse.fed, mouse.eaten))\n",
        "        mouse.eaten = 0\n",
        "        mouse.fed = 0\n",
        "\n",
        "world.display.activate(size=20)\n",
        "world.display.delay = 1\n",
        "while 1:\n",
        "    world.update(mouse.fed, mouse.eaten)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llviuq-77XS7"
      },
      "source": [
        "## 1 - Feed the Mouse without a Cat [7 points]\n",
        "\n",
        "In this task, comment the `world.addAgent(cat)` line above and run the game. If you also switch to using `world_empty.txt` instead of `world_walls.txt`, the suboptimality of the mouse implementation will be very evident.\n",
        "\n",
        "**Your task:** Improve the mouse algoritm. It should work with any world configuration (i.e., with and without walls). You are only allowed to modify the mouse class. Your goal is to minimize the time the mouse needs to get the cheese."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPyhwbAT_BCM"
      },
      "source": [
        "## 2 - Feed the Mouse with a Cat [7 points]\n",
        "\n",
        "Your mouse algorithm should also work well with a cat. So, re-include the cat into the game for this task. Again, you are only allowed to modify the mouse class.\n",
        "\n",
        "**Your task:** Your goal is to improve the number of times the mouse gets the cheese (in the presence of a cat). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB9Und0-_BPu"
      },
      "source": [
        "## 3 - Show That Your Improvements are Statistically Significant [6 points] \n",
        "\n",
        "Make sure your algorithmic improvements are statistically significant and not the results of a lucky random seed. We assume that an improvement is statistically significant, if the difference in performance is larger than 2x the standard deviation obtained in your measurements. See [Empirical Rule: Definition, Formula, Example, How It's Used](https://www.investopedia.com/terms/e/empirical-rule.asp).\n",
        "\n",
        "**Your task:** Run experiments for at least 10000 simulated steps (without showing the game grid) and plot the statistics (separately for `world_empty.txt` and `world_walls.txt`). Evaluate (1) how often does the mouse get the cheese, and (2) how long does one episode last. Compare the results before and after your improvements. We are interested in the mean and the standard deviation.\n",
        "\n",
        "Feel free to collect statistics separately and submit together with your Colab file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AXgCZ6pzoO4"
      },
      "source": [
        "## 4 - How to Submit Your Solution?\n",
        "\n",
        "Download your notebook (File --> Download --> Download .ipynb) and send per email to [saukh@tugraz.at](mailto:saukh@tugraz.at). If you run the code locally based on the GitHub sources, please send me all python files (and a short description how to run your code and how you improved the mouse strategy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlA86iBu-9RG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "llviuq-77XS7",
        "HPyhwbAT_BCM"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
